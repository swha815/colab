{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quantization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbmiY/mYEVBWvP3aRefvmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swha815/colab/blob/main/quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBqnwmHyE9yk"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<H1>Neural Network Quantization</H1>\n",
        "\n",
        "Porting floating-point (single-precision) numbers to integers (16 bits or less)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwsRvr83Iyqf"
      },
      "source": [
        "### Characteristics\n",
        "\n",
        "#### Advantages\n",
        "\n",
        "- Smaller memory footprint\n",
        "- Faster computation (roughly 5X)\n",
        "\n",
        "#### Disadvantages\n",
        "\n",
        "- Loss of critical information when not done appropriately\n",
        "- Extra burden of value conversion\n",
        "  - Weights\n",
        "    - Inference: can be pre-processed and used without modification during run-time\n",
        "    - Training: must be quantized with every update (not true for Kahan summation-based methods)\n",
        "  - Activation: input to NN must be quantized and output de-quantized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctBZqBDXJQP8"
      },
      "source": [
        "### Typical Process of NN Quantization\n",
        "\n",
        "#### Prerequisite\n",
        "\n",
        "1. Review the target HW architecture\n",
        "1. Identify where MSB/LSB truncation error (round-off and clamping) occurs\n",
        "1. Discard negligible truncation error\n",
        "1. Consider techniques to minimize truncation error (i.e. BatchNorm folding)\n",
        "\n",
        "#### Activations\n",
        "\n",
        "1. **Profile** and collect statistics from where truncation error is likely to occur (i.e. output feature map)\n",
        "1. **Analyze** gathered information and try to find/fit an appropriate distribution\n",
        "1. **Decide** quantization strategy (dependent on HW architecture)\n",
        "  - Granularity: layer-wise, channel-wise, etc\n",
        "  - Symmetry: symmetric or asymmetric\n",
        "  - Step-Size: uniform (linear) or non-uniform (quadratic or LUT-based)\n",
        "1. **Simulate** integer-based HW with _fake_ quantization layer\n",
        "\n",
        "#### Weights\n",
        "\n",
        "1. **Profile** and collect statistics from kernel\n",
        "1. **Analyze** gathered information\n",
        "1. **Decide** quantization strategy\n",
        "  - Granularity: layer-wise, output channel-wise, full channel-wise, etc\n",
        "  - Symmetry: symmetric or asymmetric\n",
        "  - Step-Size: uniform or non-uniform\n",
        "1. **Simulate** HW kernel store by replacing original weights with quantized weights\n",
        "  - Inference-only: quantize during pre-process stage\n",
        "  - Training: quantize after every update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEQvqdrTFT1Q"
      },
      "source": [
        "### Weight Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC976r3LRHn7"
      },
      "source": [
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMDlVrGjFYi8"
      },
      "source": [
        "### Activation Quantization"
      ]
    }
  ]
}