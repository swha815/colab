{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quantization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDoYj05RpPMSYx9kN7km7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swha815/colab/blob/main/quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBqnwmHyE9yk"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<H1>Neural Network Quantization</H1>\n",
        "\n",
        "Porting floating-point (single-precision) numbers to integers (16 bits or less)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwsRvr83Iyqf"
      },
      "source": [
        "### I. Characteristics\n",
        "\n",
        "#### Advantages\n",
        "\n",
        "- Smaller memory footprint\n",
        "- Faster computation (roughly 5X)\n",
        "\n",
        "#### Disadvantages\n",
        "\n",
        "- Loss of critical information when not done appropriately\n",
        "- Extra burden of value conversion\n",
        "  - Weights\n",
        "    - Inference: can be pre-processed and used without modification during run-time\n",
        "    - Training: must be quantized with every update (not true for Kahan summation-based methods)\n",
        "  - Activation: input to NN must be quantized and output de-quantized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctBZqBDXJQP8"
      },
      "source": [
        "### II. Typical Process of NN Quantization\n",
        "\n",
        "#### Prerequisite\n",
        "\n",
        "1. Review the target HW architecture\n",
        "1. Identify where MSB/LSB truncation error (round-off and clamping) occurs\n",
        "1. Discard negligible truncation error\n",
        "1. Consider techniques to minimize truncation error (i.e. BatchNorm folding)\n",
        "\n",
        "#### Weights\n",
        "\n",
        "1. **Profile** and collect statistics from kernel\n",
        "1. **Analyze** gathered information\n",
        "1. **Decide** quantization strategy\n",
        "  - Granularity: layer-wise, output channel-wise, full channel-wise, etc\n",
        "  - Symmetry: symmetric or asymmetric\n",
        "  - Step-Size: uniform or non-uniform\n",
        "1. **Simulate** HW kernel store by replacing original weights with quantized weights\n",
        "  - Inference-only: quantize during pre-process stage\n",
        "  - Training: quantize after every update\n",
        "\n",
        "#### Activations\n",
        "\n",
        "1. **Profile** and collect statistics from where truncation error is likely to occur (i.e. output feature map)\n",
        "1. **Analyze** gathered information and try to find/fit an appropriate distribution\n",
        "1. **Decide** quantization strategy (dependent on HW architecture)\n",
        "  - Granularity: layer-wise, channel-wise, etc\n",
        "  - Symmetry: symmetric or asymmetric\n",
        "  - Step-Size: uniform (linear) or non-uniform (quadratic or LUT-based)\n",
        "1. **Simulate** integer-based HW with _fake_ quantization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mASTr_LRCQr1"
      },
      "source": [
        "### III. Example\n",
        "\n",
        "Let's classify the following image.\n",
        "\n",
        "- Network: InceptionV3\n",
        "- Platform: Keras on TensorFlow\n",
        "\n",
        "![image](https://raw.githubusercontent.com/swha815/colab/main/ILSVRC2012_val_00000002.JPEG)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIjM0Clq2h6o"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.applications as keras_app\n",
        "import tensorflow.keras.preprocessing as keras_prep\n",
        "import urllib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzFn_rOm8C8Z"
      },
      "source": [
        "def print_score(prediction):\n",
        "  print('Class Scores')\n",
        "  print('=' * 50)\n",
        "\n",
        "  for p in pred[0]:\n",
        "    print('{:20}: {:7.5f}'.format(p[1], p[2]))\n",
        "\n",
        "  print('-' * 50)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nffsuKDEC9HP",
        "outputId": "b4bbf3f3-c764-4526-ebde-e2f72016901e"
      },
      "source": [
        "# Prepare model for ImageNet classification\n",
        "model = keras_app.InceptionV3(weights='imagenet')\n",
        "prep_mod = keras_app.inception_v3\n",
        "img_size = (299, 299)\n",
        "\n",
        "# Load and pre-process an image\n",
        "req = urllib.request.urlopen('https://raw.githubusercontent.com/swha815/colab/main/ILSVRC2012_val_00000002.JPEG')\n",
        "arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
        "img = org_img = cv2.imdecode(arr, -1)\n",
        "img = cv2.resize(img, img_size)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "img = prep_mod.preprocess_input(img)\n",
        "\n",
        "# Predict\n",
        "pred = model.predict(img)\n",
        "pred = keras_app.imagenet_utils.decode_predictions(pred, top=5)\n",
        "\n",
        "# Score prediction\n",
        "print_score(pred)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Scores\n",
            "==================================================\n",
            "ski                 : 0.80311\n",
            "alp                 : 0.07034\n",
            "ski_mask            : 0.00493\n",
            "mountain_tent       : 0.00260\n",
            "shovel              : 0.00147\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEQvqdrTFT1Q"
      },
      "source": [
        "### IV. Weight Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hJkz65a4ai1"
      },
      "source": [
        "#### Profile and Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjksJmTw1b_v"
      },
      "source": [
        "weight_prof_dict = dict()\n",
        "\n",
        "for layer in model.layers:\n",
        "  if not isinstance(layer, tf.keras.layers.Conv2D):\n",
        "    continue\n",
        "\n",
        "  w = layer.get_weights()\n",
        "  w_max = np.amax(w[0], axis=(0, 1))\n",
        "  w_min = np.amin(w[0], axis=(0, 1))\n",
        "\n",
        "  weight_prof_dict[layer.name] = (w_max, w_min)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIcp7VmT4MqQ"
      },
      "source": [
        "##### Decide and Simulate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3ZtXQpo5x1L"
      },
      "source": [
        "bits = 8\n",
        "signed = True\n",
        "verbose = False"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHmjGDmN4wxI"
      },
      "source": [
        "def get_int_range(bits, signed):\n",
        "  if bits <= 0 or not isinstance(bits, int):\n",
        "    raise Exception('Invalid bits specification.')\n",
        "\n",
        "  if not isinstance(signed, bool):\n",
        "    raise Exception('Invalid signed specification.')\n",
        "\n",
        "  if signed:\n",
        "    int_max = 2 ** (bits - 1) - 1\n",
        "    int_min = -(2 ** (bits - 1))\n",
        "  else:\n",
        "    int_max = 2 ** bits - 1\n",
        "    int_min = 0\n",
        "\n",
        "  return (int_max, int_min)\n",
        "\n",
        "\n",
        "def get_sf(signed, int_max, int_min, real_max, real_min):\n",
        "  if np.any(real_max < real_min):\n",
        "    raise Exception('Max is smaller than min.')\n",
        "\n",
        "  if len(real_max) != len(real_min):\n",
        "    raise Exception('real_max and real_min must be of equal lenghts.')\n",
        "\n",
        "  if not isinstance(signed, bool):\n",
        "    raise Exception('Invalid signed specification.')\n",
        "\n",
        "  if signed:\n",
        "    sf_max = np.divide(int_max, real_max,\n",
        "        out=np.ones_like(real_max), where=(real_max != 0))\n",
        "    sf_min = np.divide(int_min, real_min,\n",
        "        out=np.ones_like(real_min), where=(real_min != 0))\n",
        "    sf = np.minimum(np.abs(sf_min), np.abs(sf_max))\n",
        "  else:\n",
        "    sf = np.divide(int_max, real_max,\n",
        "        out=np.ones_like(real_max), where=(real_max != 0))\n",
        "    sf = np.abs(sf)\n",
        "\n",
        "  return sf\n",
        "\n",
        "\n",
        "def quantize_numpy(org_vals, scale_factor, int_max, int_min):\n",
        "  qvals = np.multiply(org_vals, scale_factor)\n",
        "  qvals = np.minimum(int_max, qvals)\n",
        "  qvals = np.maximum(int_min, qvals)\n",
        "  qvals = np.round(qvals)\n",
        "  qvals = np.divide(qvals, scale_factor)\n",
        "\n",
        "  return qvals\n",
        "\n",
        "\n",
        "def compress_model_param(model, bits, signed):\n",
        "  log = list()\n",
        "\n",
        "  for layer in model.layers:\n",
        "    if not layer.name in weight_prof_dict.keys():\n",
        "      continue\n",
        "\n",
        "    w = layer.get_weights()\n",
        "    w_max = weight_prof_dict[layer.name][0]\n",
        "    w_min = weight_prof_dict[layer.name][1]\n",
        "\n",
        "    # calculate scale factor\n",
        "    int_max, int_min = get_int_range(bits, signed)\n",
        "    sf = get_sf(signed, int_max, int_min, w_max, w_min)\n",
        "    \n",
        "    # quantize weights with given scale factor\n",
        "    qvals = quantize_numpy(w[0], sf, int_max, int_min)\n",
        "    quant_loss = np.sum((w[0] - qvals) ** 2)\n",
        "\n",
        "    # store quantized weights\n",
        "    w[0] = qvals\n",
        "    layer.set_weights(w)\n",
        "\n",
        "    log.append([layer.name, bits, signed, quant_loss])\n",
        "\n",
        "  return (model, log)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7H_ENy07she",
        "outputId": "dd58df4f-621f-4826-c6d3-b098de270051"
      },
      "source": [
        "# Quantize weights\n",
        "model, log = compress_model_param(model, bits, signed)\n",
        "\n",
        "if verbose == True:\n",
        "  print('Layer Parameter Loss')\n",
        "\n",
        "  for l in qlog:\n",
        "    print('  {} [{}b-{}] loss: {:.3f}'.format(l[0], l[1], l[2], l[3]))\n",
        "\n",
        "total_loss = np.sum(np.array(qlog)[:, 3].astype(float))\n",
        "print('Compressed {} layers (total loss: {:.3f})\\n'.format(len(qlog), total_loss))\n",
        "\n",
        "# Predict\n",
        "qpred = model.predict(img)\n",
        "qpred = keras_app.imagenet_utils.decode_predictions(qpred, top=5)\n",
        "\n",
        "# Score prediction\n",
        "print_score(qpred)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compressed 94 layers (total loss: 0.083)\n",
            "\n",
            "Class Scores\n",
            "==================================================\n",
            "ski                 : 0.80389\n",
            "alp                 : 0.07269\n",
            "ski_mask            : 0.00460\n",
            "mountain_tent       : 0.00270\n",
            "shovel              : 0.00144\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMDlVrGjFYi8"
      },
      "source": [
        "### Activation Quantization"
      ]
    }
  ]
}